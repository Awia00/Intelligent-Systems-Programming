\section{Introduction to implementation}
This report explains our implementation of the project. \\
Overall the project uses the classes seen in Figure \ref{fig:UMLClassDiagram}

\begin{figure}[h!]
\centering
\includegraphics[width=0.65\linewidth]{ClassDiagram.png}
\caption{UML class-diagram showing off classes implemented by group \label{fig:UMLClassDiagram}}
\end{figure}

The class \texttt{Utility9} is used to represent utility. The functions \texttt{maxValue} and \texttt{minValue} returns \texttt{Utility9} instances. It has two fields; \texttt{utiliity} to represent some utility, and a flag, \texttt{isTerminal} stating whether this is a terminal state or not. \\
Moreover, \texttt{Utility9} provides two static methods, \texttt{max(...)} and \texttt{min(...)}, that takes as arguments two \texttt{Utility9} instances, and returns the largest and the smallest, respectively; determined by comparing their utility-values. 

The \texttt{Gameboard9} class is used to encapsulate logic in relation to (the state of) and operations on the gameboard. To name a few, this includes a utility-function, a result-function and a function that returns the available actions.

Because the \texttt{GameBoard9} is used as the key in a cache for utility values, the class also overrides the \texttt{hashCode}-method and the \texttt{equals}-method. These only compares the actual game board state, which internally is represented as a 2D array of integers.

\subsection{Evaluation and cut-off function}

Turning our attention to \texttt{GameLogic9}'s method, \texttt{decideNextMove()}, we initially extract our available actions, i.e. what columns we can fill in "coins". These actions are now explored within a set time-threshold (10 seconds) \textit{and} in increasing depth, untill time runs out. We continously search for a highest utility, \texttt{currentBest}; if the utility for the current action (in the current depth) exceeds the current highest utility, \texttt{currentBest} is updated. $\alpha$ is updated to whoever's largest of $\alpha$ (initially set to $-\infty$) and \texttt{utility}.

We have implemented iterative deepening search, where the depth is calculated, based on the maximum depth of the previous search. This means that we search as far as we can within 10 seconds, and if we haven't reached the 10 second mark yet, we search through another depth. If for some reason this extra depth takes a really long time to search through, we have implemented a hard time limit on 30 seconds. This is done by having the alpha-beta-search algorithm running in a separate thread, which makes it possible for the GUI-thread (the one calling our logic) wait for the termination of the worker thread, but only if it doesn't exceed the set time limit. If this happens, the worker thread is interrupted, and the best action found so far is returned.

Futhermore the current search depth is saved, and when a new search is beginning we subtract two from this depth when starting the next search. In this way we should always be able to search through at least the same depth as was reached by the previous search. The reason that we haven't hard-coded a depth, is that some searches have a relatively small branching factor, which makes it possible to get further down in the search without exceeding the time limit.


\subsection{Heuristic}
The heuristic used in the evaluation function is based on the fact that the more coins a player has in a row, column or diagonal the better position they are in. So for each coin the player has in a line, a value is multiplied by 10, but if an opponent coin is placed anywhere in the line then the value is reduced to zero. That way only lines with a possibility of achieving a winning state are rewarded. Furthermore, if a player has 3 in a row, and there is no coin on either left or right side, and the next move on those two columns are extending the line then a situation where victory is ensured has happened and the maximum possible value is returned.

\subsection{$\alpha$-$\beta$ pruning}
The \texttt{GameLogic9} class implements an $\alpha-\beta$ pruning search algorithm. The idea is to reduce the search space, to only look at values that aren't already guaranteed to be matched, for either of min-value and max-value. The order of which values are picked have a big inpact on how well alpha beta pruning works. If the best action is picked first for max nodes and the worst for min nodes we have an optimal case and the maximum amount of nodes are cut off. To approximate this optimal order of picking actions we use the heuristic of the resulting state of each action to order the actions. 

By doing this we improve the depth the gamelogic was able to go in from around 7 to 12 in the first move.